{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a3ad63-df73-4995-ae80-dfd2996bebcb",
   "metadata": {},
   "source": [
    "Here we are using variational circuits for parity function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2cb94c-0f7c-4033-92dc-08f00c75e9ef",
   "metadata": {},
   "source": [
    "# Parity Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f8cf1c4-56b3-4134-9221-a1584c276f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "#import everything necessary\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a39e257-eb45-4816-ba99-f330fb9f2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now create quantum device\n",
    "dev = qml.device(\"default.qubit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f19a97a-c605-4a90-a973-716c213158c9",
   "metadata": {},
   "source": [
    "The steps include:\n",
    "> 1. Embedding or encoding of data\n",
    "> 2. Ansatz(e) or layout that is parametrized\n",
    "> 3. Loss and cost function for training of data\n",
    "\n",
    "We will implement quantum circuits that can be trained from labeled data to classify new data samples that's supervised machine learning. We are going to copy the parity function. \n",
    "We have a bit string x consisting of n bits. The values could be 0 or 1. Now the parity function will map this bit string to either 0 or 1 depending upon the number of 1's in x. If x contains odd numbers of 1's then y will be 1.\n",
    "\n",
    "As the input data of the parity function is binary, we will use basis encoding to encode classical data into quantum states. So the first step of state  preparation is given as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fab5f37-7e73-43a6-ae0e-4c2bb6b8a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_preparation(x):\n",
    "    qml.BasisState(x, wires=[0, 1, 2, 3])\n",
    "#why wires = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ae9e1-4dfe-48ab-901f-09ef2fb0db33",
   "metadata": {},
   "source": [
    "The step involves defining a layer. Here a layer is consisted on four qubits and it's an entangling layer. Each layer consists of  an arbitrary rotation on every qubit. Each layer is parametrized and the parameter of each layer is called weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00e0e3c8-2138-44f2-866a-89d89f4a4257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(weights):\n",
    "    for wire in range(4):\n",
    "        qml.Rot(*weights[wire], wires=wire)\n",
    "\n",
    "    for wires in ([0, 1], [1, 2], [2, 3], [3, 0]):\n",
    "        qml.CNOT(wires)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244bd622-ea57-453f-bb93-4cfa9e0be4cf",
   "metadata": {},
   "source": [
    "Now we define variational circuit as state preparation routine followed by a repetition of the layer structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cf277aa-324f-43da-8248-8435ba9b6c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def circuit(weights, x):\n",
    "    state_preparation(x)\n",
    "\n",
    "    for layer_weights in weights:\n",
    "        layer(layer_weights)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a77b9-4d95-413c-9951-e37c98dfbbcf",
   "metadata": {},
   "source": [
    "We can also add some trainable bias to our traditional circuit. The full model will be then the sum of quantum circuit + trainable bias. The bias is classical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "338b5e03-12cf-419b-8a5b-67f67b5f625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a8b1ad-1a13-4c66-b73f-61b842e352ce",
   "metadata": {},
   "source": [
    "Now coming towards training which actually involves defining loss and cost function. The loss function compares the actual and expected output. Here the standard square loss is used that measures the distance between target labels and model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bd762c7-df85-49d9-8092-6958f0957993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    # We use a call to qml.math.stack to allow subtracting the arrays directly\n",
    "    return np.mean((labels - qml.math.stack(predictions)) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b80a9e-eee7-4c9a-adac-fc2c686458d5",
   "metadata": {},
   "source": [
    "To monitor how many inputs the classifier predicted correctly, we also define the accuracy, or the proportion of predictions that agree with a set of target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4676cd58-5d2a-4ef6-a146-dcc691e053ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "    acc = sum(abs(l - p) < 1e-5 for l, p in zip(labels, predictions))\n",
    "    acc = acc / len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c00aab-9fb9-4d45-95ce-78f6b9cdfdf8",
   "metadata": {},
   "source": [
    "For learning tasks, the cost depends on the data - here the features and labels considered in the iteration of the optimization routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58b14eff-b01b-44d5-a64a-a946e0f6c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(weights, bias, X, Y):\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "    return square_loss(Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d904f76-95ca-498b-811f-d1b0b36122a6",
   "metadata": {},
   "source": [
    "Well, in analogy with the lecture, this should have been our first step. We are extracting out the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4cccdfc-85db-491b-aab7-04cbd8a0e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"parity_train.txt\", dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b131abd6-5f2e-4e1c-8e63-5df3e6315788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0 0 0 1], y = 1\n",
      "x = [0 0 1 0], y = 1\n",
      "x = [0 1 0 0], y = 1\n",
      "x = [0 1 0 1], y = -1\n",
      "x = [0 1 1 0], y = -1\n",
      "x = [0 1 1 1], y = 1\n",
      "x = [1 0 0 0], y = 1\n",
      "x = [1 0 0 1], y = -1\n",
      "x = [1 0 1 1], y = 1\n",
      "x = [1 1 1 1], y = -1\n"
     ]
    }
   ],
   "source": [
    "X = np.array(data[:, :-1])\n",
    "Y = np.array(data[:, -1])\n",
    "Y = Y * 2 - 1  # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "for x,y in zip(X, Y):\n",
    "    print(f\"x = {x}, y = {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed9d20-5014-4cc4-8227-50ac62a0e715",
   "metadata": {},
   "source": [
    "We initialize the variables randomly (but fix a seed for reproducibility). Remember that one of the variables is used as a bias, while the rest is fed into the gates of the variational circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "215c8dfb-bd12-4aaa-874c-dcb815e830c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [[[ 0.01764052  0.00400157  0.00978738]\n",
      "  [ 0.02240893  0.01867558 -0.00977278]\n",
      "  [ 0.00950088 -0.00151357 -0.00103219]\n",
      "  [ 0.00410599  0.00144044  0.01454274]]\n",
      "\n",
      " [[ 0.00761038  0.00121675  0.00443863]\n",
      "  [ 0.00333674  0.01494079 -0.00205158]\n",
      "  [ 0.00313068 -0.00854096 -0.0255299 ]\n",
      "  [ 0.00653619  0.00864436 -0.00742165]]]\n",
      "Bias:  0.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "num_qubits = 4\n",
    "num_layers = 2\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "print(\"Weights:\", weights_init)\n",
    "print(\"Bias: \", bias_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c01b09-56f2-47a6-bed7-f1f5214c3291",
   "metadata": {},
   "source": [
    "Next we create an optimizer instance and choose a batch size and run the optimizer to train our model. We track the accuracy - the share of correctly classified data samples. For this we compute the outputs of the variational classifier and turn them into predictions in $$ \n",
    " \\-1 , 1 } $$ \r\n",
    "}\r\n",
    "  by taking the sign of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a00e6b4-3524-4696-8680-0f76133e67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = NesterovMomentumOptimizer(0.5)\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "195af93c-e8bc-4eae-b2e4-3e8de261f579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:    1 | Cost: 2.3147651 | Accuracy: 0.5000000\n",
      "Iter:    2 | Cost: 1.9664866 | Accuracy: 0.5000000\n",
      "Iter:    3 | Cost: 1.9208589 | Accuracy: 0.5000000\n",
      "Iter:    4 | Cost: 2.6276126 | Accuracy: 0.5000000\n",
      "Iter:    5 | Cost: 0.9323119 | Accuracy: 0.6000000\n",
      "Iter:    6 | Cost: 1.1903549 | Accuracy: 0.5000000\n",
      "Iter:    7 | Cost: 2.0508989 | Accuracy: 0.4000000\n",
      "Iter:    8 | Cost: 1.1275531 | Accuracy: 0.6000000\n",
      "Iter:    9 | Cost: 1.1659803 | Accuracy: 0.6000000\n",
      "Iter:   10 | Cost: 1.1349618 | Accuracy: 0.6000000\n",
      "Iter:   11 | Cost: 0.9994063 | Accuracy: 0.6000000\n",
      "Iter:   12 | Cost: 1.0812559 | Accuracy: 0.6000000\n",
      "Iter:   13 | Cost: 1.2863155 | Accuracy: 0.6000000\n",
      "Iter:   14 | Cost: 2.2658259 | Accuracy: 0.4000000\n",
      "Iter:   15 | Cost: 1.1323724 | Accuracy: 0.6000000\n",
      "Iter:   16 | Cost: 1.3439737 | Accuracy: 0.8000000\n",
      "Iter:   17 | Cost: 2.0076168 | Accuracy: 0.6000000\n",
      "Iter:   18 | Cost: 1.2685760 | Accuracy: 0.5000000\n",
      "Iter:   19 | Cost: 1.6762475 | Accuracy: 0.5000000\n",
      "Iter:   20 | Cost: 1.1868237 | Accuracy: 0.6000000\n",
      "Iter:   21 | Cost: 1.4784687 | Accuracy: 0.6000000\n",
      "Iter:   22 | Cost: 1.4599473 | Accuracy: 0.6000000\n",
      "Iter:   23 | Cost: 0.9573269 | Accuracy: 0.6000000\n",
      "Iter:   24 | Cost: 1.1657424 | Accuracy: 0.5000000\n",
      "Iter:   25 | Cost: 1.0877087 | Accuracy: 0.4000000\n",
      "Iter:   26 | Cost: 1.1683687 | Accuracy: 0.6000000\n",
      "Iter:   27 | Cost: 2.1141689 | Accuracy: 0.6000000\n",
      "Iter:   28 | Cost: 1.0272966 | Accuracy: 0.5000000\n",
      "Iter:   29 | Cost: 0.9664085 | Accuracy: 0.5000000\n",
      "Iter:   30 | Cost: 1.1287654 | Accuracy: 0.6000000\n",
      "Iter:   31 | Cost: 1.4202360 | Accuracy: 0.4000000\n",
      "Iter:   32 | Cost: 1.1286000 | Accuracy: 0.5000000\n",
      "Iter:   33 | Cost: 1.9594333 | Accuracy: 0.4000000\n",
      "Iter:   34 | Cost: 1.2811832 | Accuracy: 0.4000000\n",
      "Iter:   35 | Cost: 0.8522775 | Accuracy: 0.7000000\n",
      "Iter:   36 | Cost: 1.4765281 | Accuracy: 0.6000000\n",
      "Iter:   37 | Cost: 0.9603287 | Accuracy: 0.6000000\n",
      "Iter:   38 | Cost: 1.6031314 | Accuracy: 0.6000000\n",
      "Iter:   39 | Cost: 1.1700888 | Accuracy: 0.4000000\n",
      "Iter:   40 | Cost: 1.7571779 | Accuracy: 0.4000000\n",
      "Iter:   41 | Cost: 1.9608116 | Accuracy: 0.6000000\n",
      "Iter:   42 | Cost: 2.0802752 | Accuracy: 0.6000000\n",
      "Iter:   43 | Cost: 1.1904884 | Accuracy: 0.3000000\n",
      "Iter:   44 | Cost: 0.9941585 | Accuracy: 0.6000000\n",
      "Iter:   45 | Cost: 1.0709609 | Accuracy: 0.5000000\n",
      "Iter:   46 | Cost: 0.9780625 | Accuracy: 0.6000000\n",
      "Iter:   47 | Cost: 1.1573709 | Accuracy: 0.6000000\n",
      "Iter:   48 | Cost: 1.0235239 | Accuracy: 0.6000000\n",
      "Iter:   49 | Cost: 1.2842469 | Accuracy: 0.5000000\n",
      "Iter:   50 | Cost: 0.8549226 | Accuracy: 0.6000000\n",
      "Iter:   51 | Cost: 0.5136787 | Accuracy: 1.0000000\n",
      "Iter:   52 | Cost: 0.2488031 | Accuracy: 1.0000000\n",
      "Iter:   53 | Cost: 0.0461277 | Accuracy: 1.0000000\n",
      "Iter:   54 | Cost: 0.0293518 | Accuracy: 1.0000000\n",
      "Iter:   55 | Cost: 0.0205454 | Accuracy: 1.0000000\n",
      "Iter:   56 | Cost: 0.0352514 | Accuracy: 1.0000000\n",
      "Iter:   57 | Cost: 0.0576767 | Accuracy: 1.0000000\n",
      "Iter:   58 | Cost: 0.0291305 | Accuracy: 1.0000000\n",
      "Iter:   59 | Cost: 0.0127137 | Accuracy: 1.0000000\n",
      "Iter:   60 | Cost: 0.0058108 | Accuracy: 1.0000000\n",
      "Iter:   61 | Cost: 0.0018002 | Accuracy: 1.0000000\n",
      "Iter:   62 | Cost: 0.0014089 | Accuracy: 1.0000000\n",
      "Iter:   63 | Cost: 0.0017489 | Accuracy: 1.0000000\n",
      "Iter:   64 | Cost: 0.0021282 | Accuracy: 1.0000000\n",
      "Iter:   65 | Cost: 0.0029876 | Accuracy: 1.0000000\n",
      "Iter:   66 | Cost: 0.0035331 | Accuracy: 1.0000000\n",
      "Iter:   67 | Cost: 0.0035540 | Accuracy: 1.0000000\n",
      "Iter:   68 | Cost: 0.0025639 | Accuracy: 1.0000000\n",
      "Iter:   69 | Cost: 0.0019459 | Accuracy: 1.0000000\n",
      "Iter:   70 | Cost: 0.0015856 | Accuracy: 1.0000000\n",
      "Iter:   71 | Cost: 0.0008439 | Accuracy: 1.0000000\n",
      "Iter:   72 | Cost: 0.0005960 | Accuracy: 1.0000000\n",
      "Iter:   73 | Cost: 0.0003122 | Accuracy: 1.0000000\n",
      "Iter:   74 | Cost: 0.0002446 | Accuracy: 1.0000000\n",
      "Iter:   75 | Cost: 0.0001745 | Accuracy: 1.0000000\n",
      "Iter:   76 | Cost: 0.0001215 | Accuracy: 1.0000000\n",
      "Iter:   77 | Cost: 0.0001141 | Accuracy: 1.0000000\n",
      "Iter:   78 | Cost: 0.0001538 | Accuracy: 1.0000000\n",
      "Iter:   79 | Cost: 0.0001871 | Accuracy: 1.0000000\n",
      "Iter:   80 | Cost: 0.0001330 | Accuracy: 1.0000000\n",
      "Iter:   81 | Cost: 0.0001380 | Accuracy: 1.0000000\n",
      "Iter:   82 | Cost: 0.0001336 | Accuracy: 1.0000000\n",
      "Iter:   83 | Cost: 0.0001483 | Accuracy: 1.0000000\n",
      "Iter:   84 | Cost: 0.0001234 | Accuracy: 1.0000000\n",
      "Iter:   85 | Cost: 0.0001359 | Accuracy: 1.0000000\n",
      "Iter:   86 | Cost: 0.0001268 | Accuracy: 1.0000000\n",
      "Iter:   87 | Cost: 0.0002270 | Accuracy: 1.0000000\n",
      "Iter:   88 | Cost: 0.0000865 | Accuracy: 1.0000000\n",
      "Iter:   89 | Cost: 0.0000774 | Accuracy: 1.0000000\n",
      "Iter:   90 | Cost: 0.0000759 | Accuracy: 1.0000000\n",
      "Iter:   91 | Cost: 0.0000607 | Accuracy: 1.0000000\n",
      "Iter:   92 | Cost: 0.0000523 | Accuracy: 1.0000000\n",
      "Iter:   93 | Cost: 0.0000536 | Accuracy: 1.0000000\n",
      "Iter:   94 | Cost: 0.0000444 | Accuracy: 1.0000000\n",
      "Iter:   95 | Cost: 0.0000384 | Accuracy: 1.0000000\n",
      "Iter:   96 | Cost: 0.0000497 | Accuracy: 1.0000000\n",
      "Iter:   97 | Cost: 0.0000263 | Accuracy: 1.0000000\n",
      "Iter:   98 | Cost: 0.0000229 | Accuracy: 1.0000000\n",
      "Iter:   99 | Cost: 0.0000339 | Accuracy: 1.0000000\n",
      "Iter:  100 | Cost: 0.0000174 | Accuracy: 1.0000000\n"
     ]
    }
   ],
   "source": [
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(100):\n",
    "\n",
    "    # Update the weights by one optimizer step, using only a limited batch of data\n",
    "    batch_index = np.random.randint(0, len(X), (batch_size,))\n",
    "    X_batch = X[batch_index]\n",
    "    Y_batch = Y[batch_index]\n",
    "    weights, bias = opt.step(cost, weights, bias, X=X_batch, Y=Y_batch)\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
    "\n",
    "    current_cost = cost(weights, bias, X, Y)\n",
    "    acc = accuracy(Y, predictions)\n",
    "\n",
    "    print(f\"Iter: {it+1:4d} | Cost: {current_cost:0.7f} | Accuracy: {acc:0.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d7e169-b731-446b-bd96-f3ea6dcf232c",
   "metadata": {},
   "source": [
    "Now we are going to test the algorithm on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "303993a6-9984-4637-b640-ab155f8c5bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0 0 0 0], y = -1, pred=-1.0\n",
      "x = [0 0 1 1], y = -1, pred=-1.0\n",
      "x = [1 0 1 0], y = -1, pred=-1.0\n",
      "x = [1 1 1 0], y = 1, pred=1.0\n",
      "x = [1 1 0 0], y = -1, pred=-1.0\n",
      "x = [1 1 0 1], y = 1, pred=1.0\n",
      "Accuracy on unseen data: 1.0\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"parity_test.txt\", dtype=int)\n",
    "X_test = np.array(data[:, :-1])\n",
    "Y_test = np.array(data[:, -1])\n",
    "Y_test = Y_test * 2 - 1  # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "predictions_test = [np.sign(variational_classifier(weights, bias, x)) for x in X_test]\n",
    "\n",
    "for x,y,p in zip(X_test, Y_test, predictions_test):\n",
    "    print(f\"x = {x}, y = {y}, pred={p}\")\n",
    "\n",
    "acc_test = accuracy(Y_test, predictions_test)\n",
    "print(\"Accuracy on unseen data:\", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73147af7-3b5e-42a9-a37a-3e9fff9bc410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pennylane",
   "language": "python",
   "name": "pennylane"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
